{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluating_regression_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMi8HTQE9frjloVdJ9IOnKh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rioba-Ian/Statistics/blob/main/Evaluating_regression_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAyk7018zEE5"
      },
      "source": [
        "In this notebook we shall be evaluating the metrics of a supervised machine learning model of the linear regression. \n",
        "\n",
        "We shall encounter terms such as **cost, cost function and Gradient descent.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM6mc1IazaB_"
      },
      "source": [
        "# Cost and Cost Function. \n",
        "Cost can be explained simply by this. For example our model predicts that a house should be  300,000 when we know it should start by 160,000. This difference is called the cost, the 140,000.\n",
        "\n",
        "It is how far the line is from the real daa. The best line is one that is least off from the real data. In Machine learning, cost functions are used to estimate how badly the model has performed. Simply, cost function is how wrong the model is in terms of the ability to measure the relationship between x and y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkG8vuQIA2L6"
      },
      "source": [
        "Okay, now that we understand cost and cost function, the cost function quantifies the error between the predicted values and the expected value and presents them as a single real number. Depending on the problem, the cost function can be either minimized or maximised. In minimized, the cost is returned as a loss or an error. The aim will be to find the parameters that return as small number as possible. \n",
        "\n",
        "In maximization, it shall be returning the reward. The goal will be to find the values of the parameters that return number as large as possible. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INm0MWNhBl1w"
      },
      "source": [
        "$$\n",
        "\\begin{equation}\n",
        "minimize\\frac{1}{n}\\sum_{i=1}^{n} (pred_i - y_i)^2\n",
        "\\end{equation}$$\n",
        "\n",
        "The difference between the true value and predictions is the residual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gna-JUucyxhZ"
      },
      "source": [
        "from IPython.display import Latex, Math, Image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvX2OxlHHBQp"
      },
      "source": [
        "# The MSE mean squared error\n",
        "MSE is simply the difference between the true target and the value predicted by regressor. By squaring the differences, it penalizes - gives a penalty or weight for deviating from the objective- even a small error which leads to over-estimation of how bad the model is.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "MSE = \\frac{1}{n}\\sum (y-\\hat{y})^2\n",
        "\\end{equation}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E7dXkE2Hr6P"
      },
      "source": [
        "# Root Mean Squared Error RMSE\n",
        "It is just the square root of the MSE. \n",
        "It is preferred where the errors are squared before averging which poses a high penalty on large errors, Thus, the RMSE is useful when large errors are undesired. \n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N} (y-\\hat{y})^2}{N}}\n",
        "\\end{equation}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw1bP1U2JMkg"
      },
      "source": [
        "# Mean Absolute error MAE\n",
        "MAE is the absolute difference between the target value and the value predicted by the model. It does not penlaize the errors as strongly/effectively as the MSE and is unsuitable for use-cases where we want to pay more attention to the outliers. \n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "MAE = \\frac{1}{n}\\sum |y-\\hat{y}|\n",
        "\\end{equation}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vExIpZE_QxTH"
      },
      "source": [
        "# R Squred coefficient of determination. \n",
        "R-squared is th goodness-of-fit measure for linear regression. It shows how ell the values fit compared to the original values. The higher the value, the better the model is:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "R^2 = 1- \\frac{\\sum (y-\\hat{y})^2}{\\sum (y-\\bar{y})^2}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "PS: it can be negative, meaning that we are doing worse than the mean model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtwXW04gRpYE"
      },
      "source": [
        "## Important question: What to use when?\n",
        "<img src=\"https://miro.medium.com/max/630/1*8VM2PELQ-oeM0O3ya7BIyQ.png\">\n",
        "\n",
        "I will be making most of the reference from this article below.\n",
        "\n",
        "Link is <a src=\"https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4\">here.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcsTVwY4UhNV"
      },
      "source": [
        "# The Adjusted R-squared \n",
        "it is same as the r-sqared except that it shows how well in terms of a fit of a curve adjusts for the number of terms in a model. \n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "R_{adj}^2 = 1 - [\\frac{(1-R^2)(n-1)}{n-k-1}]\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "the adj-R2 will alwats be less than or equal to the r-squared. The adj-r2 will always consider the marginal improvement added by additional terms in the model. It will thus increase or decrease based on the usefulness of the terms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ8Fb_ydBzOB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJeLOEGmV3DQ"
      },
      "source": [
        "def metrics(m, X, y):\n",
        "    yhat = m.predict(X)\n",
        "    print(yhat)\n",
        "    ss_residual = sum((y - yhat)**2)\n",
        "    ss_total = sum((y-np.mean(y))**2)\n",
        "    r_squared = 1 - (float(ss_residual))/ss_total\n",
        "    adj_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
        "    return r_squared, adj_r_squared"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ETXBLQXWnqq"
      },
      "source": [
        "df = pd.DataFrame({\"x1\": [1,2,3,4,5], \"x2\": [2.1, 4, 6.1, 8, 10.1]})\n",
        "y = np.array([2.1, 4, 6.2, 8,9])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUoJG_HHXD9K",
        "outputId": "45240d83-f90b-4edc-8616-b9f91139e944"
      },
      "source": [
        "model1 = linear_model.LinearRegression()\n",
        "model1.fit(df.drop(\"x2\", axis=1), y)\n",
        "metrics(model1, df.drop(\"x2\", axis=1), y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.3  4.08 5.86 7.64 9.42]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9854441403334162, 0.9805921871112216)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V57fPBeXsd6",
        "outputId": "d9f0ef6a-6003-45fd-a542-aafedaf6a435"
      },
      "source": [
        "model2 = linear_model.LinearRegression()\n",
        "model2.fit(df, y)\n",
        "metrics(model2, df, y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.20666667 4.22       5.76666667 7.78       9.32666667]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9874761549307456, 0.9749523098614912)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpBLLBH2YFB4",
        "outputId": "8d72d738-954c-4f9b-94cb-1a0f562b0299"
      },
      "source": [
        "model3 = linear_model.LinearRegression()\n",
        "model3.fit(df, y)\n",
        "metrics(model3, df, y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.20666667 4.22       5.76666667 7.78       9.32666667]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9874761549307456, 0.9749523098614912)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM7iil1-YXME"
      },
      "source": [
        "the table below shows that even though we are not adding any additional information from case 1 to case 2, still the r2 is increasing whereas the adj-r2 is decreasing to correct the trend; penalizing model2 for more number of varibles. \n",
        "<img src=\"https://miro.medium.com/max/630/1*C-i3nKPtHl_mkfTFgX2IQg.png\">\n",
        "Therefore we can say that, adj-r2 is better than rmse because rmse doesn't actually tell how bad a model is. \n",
        "\n",
        "Remember the misconception about going all the way to negative. Don't forget "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ProN0MqmbEYn"
      },
      "source": [
        "Let's get into something more rigorous called the Gradient. I am always learning about it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2ZW3w9ebOSn"
      },
      "source": [
        "# Gradient\n",
        "\n",
        "It means slope. a high gradient is a steeper slope. \n",
        "\n",
        "## Gradient Descent \n",
        "Cost function only gives us information of how good or bad our values are. The Gradient descent updates the values of m and b in order to reduce the cost function - minimize rmse. The idea is to start with random m and b values and the iteratively update these values in order to reach a minimum. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-X6rpDycfKS"
      },
      "source": [
        "In gradient descent, you take the current value of m and add the derivative at that, you will go down the slope. Do it a bunch of times and you will hit the bottom. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX-811HKYPZz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}